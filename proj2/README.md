This the source code for project 2 HMM of EECS 738 machine learning 
Given by Dr. Martin Kuehnhausen From (author) Ben Liu

===== Ideas ==============================================================

To describe a HMM, we need 5 parameters: 
  states, {0,1,...,N-1};
  alphabet, {'word1', 'word2', ... , 'wordm'};
  transition probability, A = {aij} = P(st+1=j|st=i), 
    the probability that the HMM jump from state i to state j at time t;
  emission probability, B = {bij}, the state i will emit word j;
  initial state probability, P = {Pi}, the probability that the first state of the HMM is state i.
  
The only hyper-parameter is states, or number of states. 
Alphabet will be determined by the dataset, A, B and P are supposed to be trained from the training data.

We apply Baum-Welch algorithm to train the HMM, which is based on the forward-backward 
algorithm, to update A, B and P. 
Once the training is done, then we apply 
1) the Viterbi algorithm to estimate the probability 
   that a given sequence is generated by the trained HMM and to predict the state emitting each word.
2) sample sequence of sepicified length from the HMM.

===== Included files ==============================================================

alllines.txt 
              training data from https://www.kaggle.com/kingburrito666/shakespeare-plays, a subset of the all lines.

HMM_log.py   
              functions that is used by HMM, it should be noted that the forward-backward algorithm 
              and Baum-Welch algorithm are scaled by taking logarithms of all probabilities 
              (and their intermidiate value). But the Viterbi algorithm is still computing i
              n the real probability domain as we want to know the probability.
                
train.py     
              the program to load data from alllines.txt and train HMM using functions in HMM_log.py, 
              call it by:
                python2 train.py
                  
hmm_parameter.txt
                the file containing all pre-trained parameter since it takes a while to train HMM
                the first line is the number of states, N, used in the HMM;
                the seconde line is the alphabet of used in the HMM, so one can choose words from here to run estimate.py;
                the third line is the initial state probability, P;
                the fourth to 4+N lines are the transition probability matrix, A;
                the rest of lines are the emission probability matrix, B.
                !!! This set of parameters are trained on a subset of shakespeare-plays, play with it with caution.
                
sample.py  
                the program will sample sequence of specified length from the trained HMM, by default the length is 10.
                call it by:
                  python2 sample.py <len>
findPath.py
                this program apply Viterbi algo to estimate the probability of the given sequence coming from the HMM, 
                and find the sequence of states(path) that emit each word in the given sentence. As mentioned above
                the states label set is {0,1,...,N-1}. 
                By default the sentence is "a thousand of his people butchered".
                call it by:
                  python2 findPath.py <sentence>
  
To make things easier, we also included a toy example, a occasionally dishonest casino:
The HMM is used to describe the process of rolling 6-side die, the dice could be either a fair dice 
or a loaded die. The loaded die has probability 0.5 of a '6' and probability 0.1 for the numbers '1' to '5'. 
If the casino is using the fair die, then the probability they are going to switch to loaded die is 0.05; 
0.1 vice versa.

rollDice.py   
                the program to simulate die sequence from the described process above.

pure_seq.txt  
                one generated sequence of length 300.

toy_train.py, toyhmm_parameter.txt
                the same as before, but for the toy example. toy_estimate.py implemented the determine algorithm
                describe above.
                
toy_estimate.py             
                this program will determine the given sequence coming from the HMM or not using Viterbi algo.
                It first estimates the probability of the given sequence. Then sample the probabilities of 
                100 sequences with the same length. If the there is a significant difference between 
                the probability of the given sequence and of the distribution, 
                then the given sequence is generated from the HMM.

===== Discussion ==========================================================================
1) We need a better way to decouple the effect of sequence length on the estiamted probability.
2) The time complexity of Baum-Welch algorithm is roughly O(N^2*m*T), where N is the number of states, m is the number of words in alphabet and T is the length of the training sequence. And profiling shows that my training has O(L^2) time complexity, where L is the number of lines in training set.
It seems expensive to apply it directly to big data set. According to Dr. Kuehnhausen, we could apply the "drop-out" strategy to emission probability to speed the training process, but not implemented now.

